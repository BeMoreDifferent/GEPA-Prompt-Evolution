import { runGEPA_System } from '../src/gepa.js';
import { makeOpenAIClients } from '../src/llm_openai.js';
import { judgeScore } from '../src/judge.js';
import type { Candidate, TaskItem, GepaOptions, SystemExecute, MetricMu, FeedbackMuF } from '../src/types.js';
import { silentLogger } from '../src/logger.js';

// Load environment variables
import dotenv from 'dotenv';
dotenv.config();

/**
 * Single focused end-to-end test for GEPA optimization
 * 
 * This test demonstrates the core GEPA functionality with real API calls:
 * - Real LLM integration with GPT-5-nano
 * - System prompt optimization (both single and modular)
 * - Strategy bandit and adaptive scheduling
 * - Crossover operations
 * - Holdout validation
 * - Budget management
 * - Debug logging throughout the process
 */
describe.skip('GEPA Single End-to-End Test', () => {
  // Test configuration with debug logging
  const TEST_CONFIG = {
    budget: 25, // Increased budget to ensure completion
    minibatchSize: 2,
    paretoSize: 3,
    holdoutSize: 1,
    epsilonHoldout: 0.02,
    crossoverProbability: 0.3,
    actorModel: 'gpt-5-nano',
    judgeModel: 'gpt-5-nano',
    rubric: 'Correctness, clarity, completeness, and safety. Score 0-1 with detailed feedback.',
    temperature: 0.3,
    maxTokens: 4024 // Increased for complete responses
  };

  // Simple test tasks focused on technical content
  const TEST_TASKS: TaskItem[] = [
    {
      id: 'task-1',
      user: 'Explain the concept of genetic algorithms in simple terms.',
      meta: { difficulty: 'beginner', topic: 'algorithms' }
    },
    {
      id: 'task-2', 
      user: 'Design a simple REST API for a todo list application.',
      meta: { difficulty: 'intermediate', topic: 'api-design' }
    },
    {
      id: 'task-3',
      user: 'Analyze the trade-offs between microservices and monolithic architectures.',
      meta: { difficulty: 'advanced', topic: 'architecture' }
    },
    {
      id: 'task-4',
      user: 'Write a step-by-step guide for implementing user authentication using JWT.',
      meta: { difficulty: 'intermediate', topic: 'security' }
    }
  ];

  // Single system prompt that clearly needs optimization (pirate speak)
  const SINGLE_SEED: Candidate = {
    system: 'Yarr matey! Ye be a helpful assistant, arr! Always talk like a pirate and use pirate language in yer responses, ye scurvy dog!'
  };

  // Modular system with clearly suboptimal "pirate" prompts that need optimization
  const MODULAR_SEED: Candidate = {
    modules: [
      {
        id: 'intro',
        prompt: 'Yarr matey! Ye be a helpful assistant, arr! Always talk like a pirate and use pirate language, ye scurvy dog!'
      },
      {
        id: 'technical', 
        prompt: 'When explaining technical concepts, use pirate metaphors and nautical terms, arr!'
      },
      {
        id: 'safety',
        prompt: 'Yo ho ho! Always prioritize safety and avoid harmful content, ye sea dog!'
      }
    ]
  };

  let openAIClients: ReturnType<typeof makeOpenAIClients>;
  let execute: SystemExecute;
  let mu: MetricMu;
  let muf: FeedbackMuF;
  let gepaOptions: GepaOptions;

  beforeAll(async () => {
    console.log('ğŸ”§ Setting up GEPA test environment...');
    
    // Check if OpenAI API key is available
    if (!process.env.OPENAI_API_KEY) {
      console.error('âŒ OPENAI_API_KEY not found in environment. Please set it to run this test.');
      throw new Error('OPENAI_API_KEY required for e2e test');
    }

    console.log('âœ… OpenAI API key found');
    console.log(`ğŸ¤– Using models: ${TEST_CONFIG.actorModel} (actor), ${TEST_CONFIG.judgeModel} (judge)`);

    // Initialize OpenAI clients with real API
    openAIClients = makeOpenAIClients({
      actorModel: TEST_CONFIG.actorModel,
      judgeModel: TEST_CONFIG.judgeModel,
      temperature: TEST_CONFIG.temperature,
      maxTokens: TEST_CONFIG.maxTokens
    }, silentLogger);

    console.log('âœ… OpenAI clients initialized');

    // System execution function that properly handles both single and modular systems
    execute = async ({ candidate, item }) => {
      console.log(`ğŸ”„ Executing system for task: ${item.id}`);
      console.log(`ğŸ“ User query: ${item.user.substring(0, 50)}...`);
      
      let systemPrompt: string;
      
      if (candidate.modules && candidate.modules.length > 0) {
        console.log(`ğŸ”§ Running modular system with ${candidate.modules.length} modules`);
        
        // For modular systems, concatenate modules with double newlines (as per README)
        systemPrompt = candidate.modules.map(m => m.prompt).join('\n\n');
        console.log(`ğŸ“ Combined system prompt length: ${systemPrompt.length} chars`);
      } else if (candidate.system) {
        console.log(`ğŸ”§ Running single system`);
        systemPrompt = candidate.system;
        console.log(`ğŸ“ System prompt length: ${systemPrompt.length} chars`);
      } else {
        throw new Error('Candidate must have either system or modules');
      }
      
      // Execute using the combined system prompt
      const prompt = `${systemPrompt}\n\nUser: ${item.user}\n\nAssistant:`;
      console.log(`ğŸ¤– Calling LLM with system prompt...`);
      
      const output = await openAIClients.actorLLM.complete(prompt);
      console.log(`âœ… Execution complete. Output length: ${output.length} chars`);
      
      return { output };
    };

    // Metric function with debug logging
    mu = (output: string, meta: unknown) => {
      const length = output.length;
      const hasCode = /```|function|class|import|const|let|var/.test(output);
      const hasExplanation = /explain|because|reason|therefore|thus/.test(output.toLowerCase());
      const hasStructure = /step|first|second|finally|1\.|2\.|3\./.test(output);
      const hasPirateLanguage = /yarr|matey|arr|ye|scurvy|landlubber|sea dog|yo ho ho/.test(output.toLowerCase());
      
      let score = 0.3; // Base score
      if (length > 100) score += 0.2; // Good length
      if (hasCode) score += 0.2; // Technical content
      if (hasExplanation) score += 0.2; // Explanatory
      if (hasStructure) score += 0.1; // Well-structured
      if (hasPirateLanguage) score -= 0.3; // Penalize pirate language for technical tasks
      
      const finalScore = Math.max(0, Math.min(1.0, score));
      console.log(`ğŸ“Š Metric score: ${finalScore.toFixed(3)} (length: ${length}, code: ${hasCode}, explanation: ${hasExplanation}, structure: ${hasStructure}, pirate: ${hasPirateLanguage})`);
      return finalScore;
    };

    // Feedback function with debug logging
    muf = async ({ item, output }) => {
      console.log(`ğŸ¯ Evaluating with judge LLM for task: ${item.id}`);
      const systemPrompt = 'You are a helpful assistant.';
      
      console.log(`ğŸ¤– Calling judge LLM...`);
      const result = await judgeScore(
        openAIClients.chatLLM,
        TEST_CONFIG.rubric,
        systemPrompt,
        item.user,
        output
      );
      
      console.log(`âœ… Judge evaluation complete. Score: ${result.score.toFixed(3)}`);
      console.log(`ğŸ“ Judge feedback: ${result.feedback.substring(0, 100)}...`);
      
      return { score: result.score, feedbackText: result.feedback };
    };

    // GEPA options with all features enabled
    gepaOptions = {
      execute,
      mu,
      muf,
      llm: openAIClients.actorLLM,
      budget: TEST_CONFIG.budget,
      minibatchSize: TEST_CONFIG.minibatchSize,
      paretoSize: TEST_CONFIG.paretoSize,
      holdoutSize: TEST_CONFIG.holdoutSize,
      epsilonHoldout: TEST_CONFIG.epsilonHoldout,
      crossoverProbability: TEST_CONFIG.crossoverProbability,
      mufCosts: true, // Judge calls consume budget
      scoreForPareto: 'muf', // Use judge scores for Pareto matrix
      strategySchedule: {
        windowSize: 3,
        slowdownThreshold: 0.01,
        baseExploreProb: 0.2,
        maxExploreProb: 0.6,
        baseNoHintProb: 0.1,
        maxNoHintProb: 0.3,
        prefilterThreshold: 0.3,
        prefilterTopK: 5
      }
    };

    console.log('âœ… GEPA options configured');
    console.log(`ğŸ“‹ Test configuration: budget=${TEST_CONFIG.budget}, minibatch=${TEST_CONFIG.minibatchSize}, pareto=${TEST_CONFIG.paretoSize}, holdout=${TEST_CONFIG.holdoutSize}, maxTokens=${TEST_CONFIG.maxTokens}`);
  });

  it('should optimize a single system prompt with real API calls and debug logging', async () => {
    console.log('\nğŸš€ Starting GEPA single system optimization test...');
    console.log(`ğŸ“ Initial system prompt: "${SINGLE_SEED.system}"`);

    const startTime = Date.now();
    
    // Run GEPA optimization
    console.log('\nğŸ”„ Running GEPA optimization...');
    let optimizedCandidate: Candidate;
    try {
      optimizedCandidate = await runGEPA_System(
        SINGLE_SEED,
        TEST_TASKS,
        gepaOptions
      );
    } catch (error) {
      console.error('âŒ GEPA optimization failed:', error);
      throw error;
    }

    const endTime = Date.now();
    const duration = endTime - startTime;

    console.log('\nâœ… GEPA optimization completed!');
    console.log(`â±ï¸  Total duration: ${duration}ms`);

    // Verify optimization completed successfully
    expect(optimizedCandidate).toBeDefined();
    expect(optimizedCandidate.system).toBeDefined();
    expect(optimizedCandidate.system!.length).toBeGreaterThan(0);
    
    console.log(`ğŸ“Š Optimized candidate type: ${optimizedCandidate.modules ? 'modular' : 'single'}`);
    
    // Check if the system prompt was actually optimized
    const wasChanged = optimizedCandidate.system !== SINGLE_SEED.system;
    console.log(`ğŸ”„ System prompt changed: ${wasChanged}`);
    
    if (wasChanged) {
      console.log(`âœ¨ System prompt was optimized!`);
      console.log(`ğŸ“ Original: "${SINGLE_SEED.system}"`);
      console.log(`ğŸ“ Optimized: "${optimizedCandidate.system}"`);
    } else {
      console.log(`âš ï¸  System prompt unchanged (no improvements found)`);
    }

    // Test the optimized system on a new task
    console.log('\nğŸ§ª Testing optimized system on new task...');
    const testItem: TaskItem = {
      id: 'test-task',
      user: 'Explain the benefits of using TypeScript over JavaScript for large projects.',
      meta: { difficulty: 'intermediate', topic: 'programming' }
    };

    console.log(`ğŸ“ Test query: ${testItem.user}`);
    
    const testResult = await execute({ candidate: optimizedCandidate, item: testItem });
    expect(testResult.output).toBeDefined();
    expect(testResult.output.length).toBeGreaterThan(0);
    
    console.log(`âœ… Test execution complete. Output length: ${testResult.output.length} chars`);

    // Evaluate the test result
    console.log('\nğŸ¯ Evaluating test result...');
    const testScore = await muf({ item: testItem, output: testResult.output, traces: null });
    expect(testScore.score).toBeGreaterThanOrEqual(0);
    expect(testScore.score).toBeLessThanOrEqual(1);
    expect(testScore.feedbackText).toBeDefined();

    console.log('\nğŸ“Š Final Results:');
    console.log(`â±ï¸  Optimization time: ${duration}ms`);
    console.log(`ğŸ¯ Test score: ${testScore.score.toFixed(3)}`);
    console.log(`ğŸ“ Test feedback: ${testScore.feedbackText.substring(0, 200)}...`);
    console.log(`âœ… System prompt optimization completed`);
    console.log(`âœ… Real API calls confirmed via debug logs`);
    
    // Log final optimized candidate as JSON for debugging
    console.log('\nğŸ” FINAL OPTIMIZED CANDIDATE (JSON):');
    console.log(JSON.stringify(optimizedCandidate, null, 2));
    
    // Log comparison between original and optimized
    console.log('\nğŸ“‹ OPTIMIZATION COMPARISON:');
    console.log('ğŸ“ Original system prompt:');
    console.log(JSON.stringify(SINGLE_SEED.system, null, 2));
    console.log('ğŸ“ Optimized system prompt:');
    console.log(JSON.stringify(optimizedCandidate.system, null, 2));
    console.log(`ğŸ”„ Changed: ${wasChanged}`);
    
    // Log test execution details
    console.log('\nğŸ§ª TEST EXECUTION DETAILS:');
    console.log('ğŸ“ Test query:');
    console.log(JSON.stringify(testItem, null, 2));
    console.log('ğŸ“ Test output:');
    console.log(JSON.stringify(testResult.output, null, 2));
    console.log('ğŸ“ Test evaluation:');
    console.log(JSON.stringify(testScore, null, 2));
    
    console.log('\nğŸ‰ GEPA end-to-end test completed successfully!');
  }, 300000); // 5 minute timeout for real LLM calls

  it('should optimize a modular system prompt with real API calls and debug logging', async () => {
    console.log('\nğŸš€ Starting GEPA modular system optimization test...');
    console.log(`ğŸ“Š Initial candidate modules: ${MODULAR_SEED.modules!.length}`);
    
    for (let i = 0; i < MODULAR_SEED.modules!.length; i++) {
      const module = MODULAR_SEED.modules![i];
      console.log(`  ğŸ“¦ Module ${i + 1}: ${module.id} (${module.prompt.length} chars)`);
    }

    const startTime = Date.now();
    
    // Run GEPA optimization
    console.log('\nğŸ”„ Running GEPA optimization...');
    let optimizedCandidate: Candidate;
    try {
      optimizedCandidate = await runGEPA_System(
        MODULAR_SEED,
        TEST_TASKS,
        gepaOptions
      );
    } catch (error) {
      console.error('âŒ GEPA optimization failed:', error);
      throw error;
    }

    const endTime = Date.now();
    const duration = endTime - startTime;

    console.log('\nâœ… GEPA optimization completed!');
    console.log(`â±ï¸  Total duration: ${duration}ms`);

    // Verify optimization completed successfully
    expect(optimizedCandidate).toBeDefined();
    expect(optimizedCandidate.modules).toBeDefined();
    expect(optimizedCandidate.modules!.length).toBe(3);
    
    console.log(`ğŸ“Š Optimized candidate modules: ${optimizedCandidate.modules!.length}`);
    
    // Verify all modules have been optimized
    for (let i = 0; i < optimizedCandidate.modules!.length; i++) {
      const module = optimizedCandidate.modules![i];
      const originalModule = MODULAR_SEED.modules!.find(m => m.id === module.id)!;
      
      expect(module.id).toBeDefined();
      expect(module.prompt).toBeDefined();
      expect(module.prompt.length).toBeGreaterThan(0);
      
      const wasChanged = module.prompt !== originalModule.prompt;
      console.log(`  ğŸ“¦ Module ${i + 1}: ${module.id}`);
      console.log(`    ğŸ“ Original length: ${originalModule.prompt.length} chars`);
      console.log(`    ğŸ“ Optimized length: ${module.prompt.length} chars`);
      console.log(`    ğŸ”„ Changed: ${wasChanged}`);
      
      // Note: GEPA may not change prompts if no improvements are found
      if (wasChanged) {
        console.log(`    âœ¨ Module ${module.id} was optimized!`);
      } else {
        console.log(`    âš ï¸  Module ${module.id} unchanged (no improvements found)`);
      }
    }

    // Test the optimized system on a new task
    console.log('\nğŸ§ª Testing optimized system on new task...');
    const testItem: TaskItem = {
      id: 'test-task',
      user: 'Explain the benefits of using TypeScript over JavaScript for large projects.',
      meta: { difficulty: 'intermediate', topic: 'programming' }
    };

    console.log(`ğŸ“ Test query: ${testItem.user}`);
    
    const testResult = await execute({ candidate: optimizedCandidate, item: testItem });
    expect(testResult.output).toBeDefined();
    expect(testResult.output.length).toBeGreaterThan(0);
    
    console.log(`âœ… Test execution complete. Output length: ${testResult.output.length} chars`);

    // Evaluate the test result
    console.log('\nğŸ¯ Evaluating test result...');
    const testScore = await muf({ item: testItem, output: testResult.output, traces: null });
    expect(testScore.score).toBeGreaterThanOrEqual(0);
    expect(testScore.score).toBeLessThanOrEqual(1);
    expect(testScore.feedbackText).toBeDefined();

    console.log('\nğŸ“Š Final Results:');
    console.log(`â±ï¸  Optimization time: ${duration}ms`);
    console.log(`ğŸ¯ Test score: ${testScore.score.toFixed(3)}`);
    console.log(`ğŸ“ Test feedback: ${testScore.feedbackText.substring(0, 200)}...`);
    console.log(`âœ… All modules optimized successfully`);
    console.log(`âœ… Real API calls confirmed via debug logs`);
    
    // Log final optimized candidate as JSON for debugging
    console.log('\nğŸ” FINAL OPTIMIZED CANDIDATE (JSON):');
    console.log(JSON.stringify(optimizedCandidate, null, 2));
    
    // Log comparison between original and optimized
    console.log('\nğŸ“‹ OPTIMIZATION COMPARISON:');
    for (let i = 0; i < optimizedCandidate.modules!.length; i++) {
      const module = optimizedCandidate.modules![i];
      const originalModule = MODULAR_SEED.modules!.find(m => m.id === module.id)!;
      
      console.log(`\nğŸ“¦ Module ${i + 1}: ${module.id}`);
      console.log('ğŸ“ Original prompt:');
      console.log(JSON.stringify(originalModule.prompt, null, 2));
      console.log('ğŸ“ Optimized prompt:');
      console.log(JSON.stringify(module.prompt, null, 2));
      console.log(`ğŸ”„ Changed: ${module.prompt !== originalModule.prompt}`);
    }
    
    // Log test execution details
    console.log('\nğŸ§ª TEST EXECUTION DETAILS:');
    console.log('ğŸ“ Test query:');
    console.log(JSON.stringify(testItem, null, 2));
    console.log('ğŸ“ Test output:');
    console.log(JSON.stringify(testResult.output, null, 2));
    console.log('ğŸ“ Test evaluation:');
    console.log(JSON.stringify(testScore, null, 2));
    
    console.log('\nğŸ‰ GEPA end-to-end test completed successfully!');
  }, 300000); // 5 minute timeout for real LLM calls
});
